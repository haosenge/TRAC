{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1, 2, 3\"\n",
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "from kilt.retrievers import DPR_connector\n",
    "import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = DPR_connector.DPR.from_config_file(\n",
    "    \"dpr\", \"kilt/configs/retriever/default_dpr.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataset and get\n",
    "- query\n",
    "- golden passage titles\n",
    "- retrieved passages\n",
    "- answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Natural Questions', 'TriviaQA', 'FEVER']\n",
    "task = 'Natural Questions'\n",
    "dataset = tasks.RQA(task=task)\n",
    "retriever.feed_data(dataset.query_data)\n",
    "provenance = retriever.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data, validated_data, elements = \\\n",
    "    dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "test_elements = utils.split(elements, test_indices)\n",
    "elements = utils.split(elements, cal_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "answers = []\n",
    "retrieved_texts = []\n",
    "retrieved_scores = []\n",
    "for element in elements:\n",
    "    query_id = element['id']\n",
    "    query = element['input']\n",
    "    answer = [ans['answer'] for ans in element['output'] if \"answer\" in ans]\n",
    "    wiki_id = [[wiki['wikipedia_id'] for wiki in ans['provenance']] for ans in element['output'] if 'provenance' in ans]\n",
    "    wiki_title = [[wiki['title'] for wiki in ans['provenance']] for ans in element['output'] if 'provenance' in ans]\n",
    "    ids = []\n",
    "    for id in wiki_id:\n",
    "        ids.extend(id)\n",
    "    retrieved = provenance[query_id]\n",
    "    retrieved_id = [ans['wikipedia_id'] for ans in retrieved]\n",
    "    retrieved_title = [ans['wikipedia_title'] for ans in retrieved]\n",
    "    retrieved_text = [ans['text'] for ans in retrieved]\n",
    "    convert = utils.convert_list_to_dict(retrieved)\n",
    "    score = [convert[id] for id in convert if id in ids]\n",
    "    if len(score) == 0:\n",
    "        continue\n",
    "    \n",
    "    queries.append(query)\n",
    "    answers.append(answer)\n",
    "    retrieved_texts.append(retrieved_text)\n",
    "    retrieved_scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup semantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup open source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensource\n",
    "import importlib\n",
    "importlib.reload(opensource)\n",
    "model, pipeline, tokenizer = opensource.setup_openmodel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup prompt and ask open source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_true_scores = []\n",
    "with torch.no_grad():\n",
    "    for idx, (query, answer, contexts, score) \\\n",
    "        in enumerate(zip(queries, answers, retrieved_texts, retrieved_scores)):\n",
    "        \n",
    "        if idx > 2:\n",
    "            break\n",
    "        \n",
    "        true_scores_tmp = []\n",
    "        for context, s in zip(contexts, score):\n",
    "            prompt = utils.get_prompt_template(query, \"\", task='Natural Questions')\n",
    "            sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer)\n",
    "            generated_texts = []\n",
    "            for seq in sequences:\n",
    "                generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "\n",
    "            if semantic:\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.compute_semantic_clusterring(\n",
    "                        semantic_model, \n",
    "                        semantic_tokenizer,\n",
    "                        prompt,\n",
    "                        generated_texts,\n",
    "                    )\n",
    "            else:\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.compute_keyword_clusterring(\n",
    "                        generated_texts,\n",
    "                        scorer\n",
    "                    )\n",
    "            true_scores, matched_answers = utils.processing_answers(\n",
    "                semantic_set_ids, semantic_probs, \n",
    "                item_occurance, answer, scorer,\n",
    "                threshold=0.3\n",
    "            )\n",
    "            true_scores_tmp.extend(true_scores)\n",
    "        opensource_true_scores.append(true_scores_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup chatgpt and ask chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = True\n",
    "chatgpt_true_scores = []\n",
    "for idx, (query, answer, contexts, score) \\\n",
    "    in enumerate(zip(queries, answers, retrieved_texts, retrieved_scores)):\n",
    "    \n",
    "    if idx > 2:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    true_scores_tmp = []\n",
    "    for context, s in zip(contexts, score):\n",
    "        \n",
    "        prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "        if chat:\n",
    "            sequences = utils.ask_chatgpt(prompt)\n",
    "        else:\n",
    "            sequences, probs = utils.ask_chatgpt(prompt)\n",
    "            \n",
    "        if semantic:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_semantic_clusterring(\n",
    "                    semantic_model, \n",
    "                    semantic_tokenizer,\n",
    "                    prompt,\n",
    "                    sequences,\n",
    "                )\n",
    "        else:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_keyword_clusterring(\n",
    "                    sequences,\n",
    "                    scorer\n",
    "                )\n",
    "        true_scores, matched_answer = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        true_scores_tmp.extend(true_scores)\n",
    "    chatgpt_true_scores.append(true_scores_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- retrieved_scores: true scores for retriever\n",
    "- opensource_true_scores: true scores for open source model\n",
    "- chatgpt_true_scores: true scores for chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(opensource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute threshold on calibration set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_threshold = utils.compute_threshold(retrieved_scores, alpha=0.025, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_thr_qa = utils.compute_threshold(opensource_true_scores, alpha=0.025, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_thr_qa = utils.compute_threshold(chatgpt_true_scores, alpha=0.025, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate thresholds on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "answers = []\n",
    "retrieved_texts = []\n",
    "covered = []\n",
    "for element in test_elements:\n",
    "    query_id = element['id']\n",
    "    query = element['input']\n",
    "    answer = [ans['answer'] for ans in element['output'] if \"answer\" in ans]\n",
    "    wiki_id = [[wiki['wikipedia_id'] for wiki in ans['provenance']] for ans in element['output'] if 'provenance' in ans]\n",
    "    wiki_title = [[wiki['title'] for wiki in ans['provenance']] for ans in element['output'] if 'provenance' in ans]\n",
    "    ids = []\n",
    "    for id in wiki_id:\n",
    "        ids.extend(id)\n",
    "    all_retrieved = [r for r in provenance[query_id]]\n",
    "    all_id = [ans['wikipedia_id'] for ans in all_retrieved]\n",
    "    retrieved = [r for r in provenance[query_id] if float(r['score']) >= retrieved_threshold]\n",
    "    retrieved_id = [ans['wikipedia_id'] for ans in retrieved]\n",
    "    retrieved_title = [ans['wikipedia_title'] for ans in retrieved]\n",
    "    retrieved_text = [ans['text'] for ans in retrieved]\n",
    "    \n",
    "    if len(utils.intersection(all_id, ids)) == 0:\n",
    "        continue\n",
    "    \n",
    "    covered.append(len(utils.intersection(retrieved_id, ids))>=1)\n",
    "    \n",
    "    queries.append(query)\n",
    "    answers.append(answer)\n",
    "    retrieved_texts.append(retrieved_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coverage rate\", np.mean(covered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_covered = []\n",
    "opensource_thr_qa = 0.5\n",
    "with torch.no_grad():\n",
    "    for idx, (query, answer, contexts) \\\n",
    "        in enumerate(zip(queries, answers, retrieved_texts)):\n",
    "        \n",
    "        if idx > 3:\n",
    "            break\n",
    "        \n",
    "        cover = False\n",
    "        for context, s in zip(contexts, score):\n",
    "            prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "            sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer)\n",
    "            generated_texts = []\n",
    "            for seq in sequences:\n",
    "                generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "\n",
    "            if semantic:\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.compute_semantic_clusterring(\n",
    "                        semantic_model, \n",
    "                        semantic_tokenizer,\n",
    "                        prompt,\n",
    "                        generated_texts,\n",
    "                    )\n",
    "            else:\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.compute_keyword_clusterring(\n",
    "                        generated_texts,\n",
    "                        scorer\n",
    "                    )\n",
    "            true_scores, matched_answers = utils.processing_answers(\n",
    "                semantic_set_ids, semantic_probs, \n",
    "                item_occurance, answer, scorer,\n",
    "                threshold=0.3, thr_qa=opensource_thr_qa\n",
    "            )\n",
    "            if len(true_scores) >= 1:\n",
    "                cover = True\n",
    "        opensource_covered.append(cover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coverage rate\", np.mean(opensource_covered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/biencoder-nq-dev.json\", \"r\") as fin:\n",
    "            nq_dpr = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dpr[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/nq-dev-kilt.jsonl\", \"r\") as fin:\n",
    "            nq_kilt = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RQA_dpr:\n",
    "    def __init__(self, task='nq') -> None:\n",
    "        self.task = task\n",
    "        self.query_data, self.validated_data, self.elements = self.load_dataset()\n",
    "    \n",
    "    def load_dataset(self) -> None:\n",
    "        with open(\"data/biencoder-nq-dev.json\", \"r\") as fin:\n",
    "            nq_dpr = json.load(fin)\n",
    "        \n",
    "        elements = []\n",
    "        query_data = []\n",
    "        validated_data = {}\n",
    "        for idx, record in enumerate(nq_dpr):\n",
    "            elements.append(record)\n",
    "            validated_data[idx] = record\n",
    "            query_data.append(\n",
    "                {\"query\": record[\"question\"], \"id\": idx}\n",
    "            )\n",
    "        return query_data, validated_data, elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dpr = RQA_dpr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_dpr.query_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.feed_data(dataset.query_data[:500])\n",
    "provenance_dpr = retriever.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_dpr.elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "test_elements = utils.split(dataset_dpr.elements, test_indices)\n",
    "elements_dpr = utils.split(dataset_dpr.elements, cal_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "answers = []\n",
    "retrieved_texts = []\n",
    "retrieved_scores = []\n",
    "passages = []\n",
    "for query_id, element in zip(cal_indices.tolist(), elements_dpr):\n",
    "    # extract data information\n",
    "    query = element['question']\n",
    "    answer = [ans for ans in element['answers']]\n",
    "    passage_id = [ctx['passage_id'] for ctx in element['positive_ctxs']]\n",
    "    passage_title = [ctx['title'] for ctx in element['positive_ctxs']]\n",
    "    passage_text = [ctx['text'] for ctx in element['positive_ctxs']]\n",
    "    \n",
    "#     retrieved = provenance_dpr[query_id]\n",
    "#     retrieved_id = [ans['wikipedia_id'] for ans in retrieved]\n",
    "#     retrieved_title = [ans['wikipedia_title'] for ans in retrieved]\n",
    "#     retrieved_text = [ans['text'] for ans in retrieved]\n",
    "#     convert = utils.convert_list_to_dict(retrieved)\n",
    "#     score = [convert[id] for id in convert if id in ids]\n",
    "#     if len(score) == 0:\n",
    "#         continue\n",
    "    \n",
    "    queries.append(query)\n",
    "    answers.append(answer)\n",
    "    passages.append(passage_text)\n",
    "#     retrieved_texts.append(retrieved_text)\n",
    "#     retrieved_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_true_scores = []\n",
    "with torch.no_grad():\n",
    "    for idx, (query, answer) in enumerate(zip(queries, answers)):\n",
    "    \n",
    "        if idx > 2:\n",
    "            break\n",
    "        \n",
    "        if chat:\n",
    "            sequences = utils.ask_chatgpt(query)\n",
    "        else:\n",
    "            sequences, probs = utils.ask_chatgpt(query)\n",
    "            \n",
    "        if semantic:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_semantic_clusterring(\n",
    "                    semantic_model, \n",
    "                    semantic_tokenizer,\n",
    "                    prompt,\n",
    "                    sequences,\n",
    "                )\n",
    "        else:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_keyword_clusterring(\n",
    "                    sequences,\n",
    "                    scorer\n",
    "                )\n",
    "        true_scores, matched_answer = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        chatgpt_true_scores.append(true_scores_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_set_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_true_scores = []\n",
    "with torch.no_grad():\n",
    "    for idx, (query, answer, passage) in enumerate(zip(queries, answers, passages)):\n",
    "    \n",
    "        if idx > 2:\n",
    "            break\n",
    "        \n",
    "        query = utils.get_prompt_template(query, passage[0], task='Natural Questions')\n",
    "        if chat:\n",
    "            sequences = utils.ask_chatgpt(query)\n",
    "        else:\n",
    "            sequences, probs = utils.ask_chatgpt(query)\n",
    "            \n",
    "        if semantic:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_semantic_clusterring(\n",
    "                    semantic_model, \n",
    "                    semantic_tokenizer,\n",
    "                    prompt,\n",
    "                    sequences,\n",
    "                )\n",
    "        else:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_keyword_clusterring(\n",
    "                    sequences,\n",
    "                    scorer\n",
    "                )\n",
    "        true_scores, matched_answer = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        chatgpt_true_scores.append(true_scores_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
