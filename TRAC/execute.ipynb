{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "from kilt.retrievers import DPR_connector\n",
    "import utils\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/lishuo1/retriever_uncertainty/TRAC/utils.py'>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = DPR_connector.DPR.from_config_file(\n",
    "    \"dpr\", \"kilt/configs/retriever/default_dpr.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataset and get\n",
    "- query\n",
    "- golden passage titles\n",
    "- retrieved passages\n",
    "- answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Natural Questions', 'TriviaQA', 'FEVER']\n",
    "task = 'Natural Questions'\n",
    "dataset = tasks.RQA(task=task)\n",
    "retriever.feed_data(dataset.query_data)\n",
    "provenance = retriever.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data, validated_data, elements = \\\n",
    "    dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "answers = []\n",
    "retrieved_texts = []\n",
    "scores = []\n",
    "for element in elements:\n",
    "    query_id = element['id']\n",
    "    query = element['input']\n",
    "    answer = [ans['answer'] for ans in element['output'] if \"answer\" in ans]\n",
    "    wiki_id = [[wiki['wikipedia_id'] for wiki in ans['provenance']] for ans in element['output'] if 'provenance' in ans]\n",
    "    wiki_title = [[wiki['title'] for wiki in ans['provenance']] for ans in element['output'] if 'provenance' in ans]\n",
    "    ids = []\n",
    "    for id in wiki_id:\n",
    "        ids.extend(id)\n",
    "    retrieved = provenance[query_id]\n",
    "    retrieved_id = [ans['wikipedia_id'] for ans in retrieved]\n",
    "    retrieved_title = [ans['wikipedia_title'] for ans in retrieved]\n",
    "    retrieved_text = [ans['text'] for ans in retrieved]\n",
    "    convert = utils.convert_list_to_dict(retrieved)\n",
    "    score = [convert[id] for id in convert if id in ids]\n",
    "    if len(score) == 0:\n",
    "        continue\n",
    "    \n",
    "    queries.append(query)\n",
    "    answers.append(answer)\n",
    "    retrieved_texts.append(retrieved_text)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup semantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[140665223045504] 2023-09-18 13:12:48,250 [INFO] absl: Using default tokenizer.\n"
     ]
    }
   ],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup open source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline, tokenizer = opensource.setup_openmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'opensource' from '/home/lishuo1/retriever_uncertainty/TRAC/opensource.py'>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(opensource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup prompt and ask open source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'therefore': 0, 'Therefore': 0, 'Triangle': 2, 'triangle': 2}\n"
     ]
    }
   ],
   "source": [
    "for query, answer, contexts, score \\\n",
    "    in zip(queries, answers, retrieved_texts, scores):\n",
    "    \n",
    "    for context, s in zip(contexts, score):\n",
    "        prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "        sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer)\n",
    "        generated_texts = []\n",
    "        for seq in sequences:\n",
    "            generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "        \n",
    "        if semantic:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_semantic_clusterring(\n",
    "                    semantic_model, \n",
    "                    semantic_tokenizer,\n",
    "                    prompt,\n",
    "                    generated_texts,\n",
    "                )\n",
    "        else:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_keyword_clusterring(\n",
    "                    generated_texts,\n",
    "                    scorer\n",
    "                )\n",
    "        print(semantic_set_ids)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup chatgpt and ask chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[140665223045504] 2023-09-18 13:10:38,788 [INFO] absl: Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conclusion\n",
      "consequence\n",
      "conclusion\n",
      "consequence\n",
      "conclusion\n",
      "{'conclusion': 0, 'consequence': 1}\n"
     ]
    }
   ],
   "source": [
    "chat = True\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()\n",
    "    \n",
    "for query, answer, contexts, score \\\n",
    "    in zip(queries, answers, retrieved_texts, scores):\n",
    "    for context, s in zip(contexts, score):\n",
    "        \n",
    "        prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "        if chat:\n",
    "            sequences = utils.ask_chatgpt(prompt)\n",
    "        else:\n",
    "            sequences, probs = utils.ask_chatgpt(prompt)\n",
    "    \n",
    "        for seq in sequences:\n",
    "            print(seq)\n",
    "            \n",
    "        if semantic:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_semantic_clusterring(\n",
    "                    semantic_model, \n",
    "                    semantic_tokenizer,\n",
    "                    prompt,\n",
    "                    sequences,\n",
    "                )\n",
    "        else:\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.compute_keyword_clusterring(\n",
    "                    sequences,\n",
    "                    scorer\n",
    "                )\n",
    "        print(semantic_set_ids)\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
