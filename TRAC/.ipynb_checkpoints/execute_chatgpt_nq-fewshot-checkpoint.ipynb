{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1, 2, 3\"\n",
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Found cached dataset wiki_dpr (/home/lishuo1/.cache/huggingface/datasets/wiki_dpr/psgs_w100.multiset.compressed/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "wiki = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='nq'\n",
    "dataset_dpr = tasks.RQA_dpr(task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_dpr.elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "elements = dataset_dpr.elements\n",
    "query = [element['question'] for element in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "scores, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "#         print('Done writing list into a binary file')\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "def save_results(task):\n",
    "    # save retrieved_scores to a pickle file\n",
    "    write_list(retrieved_scores, f'chatgpt_retrieved_scores_{task}_fewshot.p')\n",
    "    # save retrieved_true_scores to a pickle file\n",
    "    write_list(retrieved_true_scores, f'chatgpt_retrieved_true_scores_{task}_fewshot.p')\n",
    "    # save queries to a pickle file\n",
    "    write_list(queries, f'chatgpt_queries_{task}_fewshot.p')\n",
    "    # save answers to a pickle file\n",
    "    write_list(answers, f'chatgpt_true_answers_{task}_fewshot.p')\n",
    "    # save passages to a pickle file\n",
    "    write_list(passages, f'chatgpt_passages_{task}_fewshot.p')\n",
    "    # save chatgpt_true_scores to a pickle file\n",
    "    write_list(chatgpt_true_scores, f'chatgpt_true_scores_{task}_fewshot.p')\n",
    "    # save chatgpt_texts to a pickle file\n",
    "#     write_list(chatgpt_texts, f'chatgpt_texts_{task}.p')\n",
    "    # save chatgpt_answers to a pickle file\n",
    "    write_list(chatgpt_answers, f'chatgpt_answers_{task}_fewshot.p')\n",
    "    # save chatgpt_semantics to a picle file\n",
    "    write_list(chatgpt_semantics, f'chatgpt_semantics_{task}_fewshot.p')\n",
    "    # save feasibilities to a pickle file\n",
    "    write_list(feasibilities, f'chatgpt_feasibilities_{task}_fewshot.p')\n",
    "    # save occurances to a pickle file\n",
    "    write_list(occurances, f'chatgpt_occurances_{task}_fewshot.p')\n",
    "    # save semantic_ids to a pickle file\n",
    "    write_list(semantic_ids, f'chatgpt_semantic_ids_{task}_fewshot.p')\n",
    "    # save probs to a picle file\n",
    "    write_list(probs, f'chatgpt_probs_{task}_fewshot.p')\n",
    "    \n",
    "    write_list(retrieved_scores_unc, f'chatgpt_retrieved_scores_unc_{task}_fewshot.p')\n",
    "    write_list(retrieved_true_scores_unc, f'chatgpt_retrieved_true_scores_unc_{task}_fewshot.p')\n",
    "    write_list(queries_unc, f'chatgpt_queries_unc_{task}_fewshot.p')\n",
    "    write_list(answers_unc, f'chatgpt_answers_unc_{task}_fewshot.p')\n",
    "    write_list(passages_unc, f'chatgpt_passages_unc_{task}_fewshot.p')\n",
    "    write_list(chatgpt_true_scores_unc, f'chatgpt_true_scores_unc_{task}_fewshot.p')\n",
    "    write_list(chatgpt_answers_unc, f'chatgpt_answers_unc_{task}_fewshot.p')\n",
    "    write_list(occurances_unc, f'chatgpt_occurances_unc_{task}_fewshot.p')\n",
    "    write_list(semantic_ids_unc, f'chatgpt_semantic_ids_unc_{task}_fewshot.p')\n",
    "    write_list(probs_unc, f'chatgpt_probs_unc_{task}_fewshot.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(task):\n",
    "    retrieved_scores = read_list(f'chatgpt_retrieved_scores_{task}_fewshot.p')\n",
    "    retrieved_true_scores = read_list(f'chatgpt_retrieved_true_scores_{task}_fewshot.p')\n",
    "    queries = read_list(f'chatgpt_queries_{task}_fewshot.p')\n",
    "    answers = read_list(f'chatgpt_answers_{task}_fewshot.p')\n",
    "    chatgpt_true_scores = read_list(f'chatgpt_true_scores_{task}_fewshot.p')\n",
    "    chatgpt_answers = read_list(f'chatgpt_answers_{task}_fewshot.p')\n",
    "    chatgpt_passages = read_list(f'chatgpt_passages_{task}_fewshot.p')\n",
    "    chatgpt_semantics = read_list(f'chatgpt_semantics_{task}_fewshot.p')\n",
    "    chatgpt_occurances = read_list(f'chatgpt_occurances_{task}_fewshot.p')\n",
    "    chatgpt_semantic_ids = read_list(f'chatgpt_semantic_ids_{task}_fewshot.p')\n",
    "    chatgpt_probs = read_list(f'chatgpt_probs_{task}_fewshot.p')\n",
    "    \n",
    "    retrieved_scores_unc = read_list(f'chatgpt_retrieved_scores_unc_{task}_fewshot.p')\n",
    "    retrieved_true_scores_unc = read_list(f'chatgpt_retrieved_true_scores_unc_{task}_fewshot.p')\n",
    "    queries_unc = read_list(f'chatgpt_queries_unc_{task}_fewshot.p')\n",
    "    answers_unc = read_list(f'chatgpt_answers_unc_{task}_fewshot.p')\n",
    "    passages_unc = read_list(f'chatgpt_passages_unc_{task}_fewshot.p')\n",
    "    chatgpt_true_scores_unc = read_list(f'chatgpt_true_scores_unc_{task}_fewshot.p')\n",
    "    chatgpt_answers_unc = read_list(f'chatgpt_answers_unc_{task}_fewshot.p')\n",
    "    chatgpt_occurances_unc = read_list(f'chatgpt_occurances_unc_{task}_fewshot.p')\n",
    "    chatgpt_semantic_ids_unc = read_list(f'chatgpt_semantic_ids_unc_{task}_fewshot.p')\n",
    "    chatgpt_probs_unc = read_list(f'chatgpt_probs_unc_{task}_fewshot.p')\n",
    "    \n",
    "    return retrieved_scores, retrieved_true_scores, queries, answers, chatgpt_true_scores, chatgpt_answers, chatgpt_passages, chatgpt_semantics, chatgpt_occurances, chatgpt_semantic_ids, chatgpt_probs, retrieved_scores_unc, retrieved_true_scores_unc, queries_unc, answers_unc, passages_unc, chatgpt_true_scores_unc, chatgpt_answers_unc, chatgpt_occurances_unc, chatgpt_semantic_ids_unc, chatgpt_probs_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct demonstrations\n",
    "demonstrations = []\n",
    "for element in elements[-2:]:\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element, dataset=task)\n",
    "    demonstrations.append([query, passage_text[0], answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/lishuo1/retriever_uncertainty/TRAC/utils.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "CPU times: user 4.57 ms, sys: 191 µs, total: 4.76 ms\n",
      "Wall time: 4.92 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chat = True\n",
    "semantic = False\n",
    "queries = []\n",
    "answers = []\n",
    "passages = []\n",
    "retrieved_scores = []\n",
    "retrieved_true_scores = []\n",
    "chatgpt_true_scores = []\n",
    "chatgpt_texts = []\n",
    "chatgpt_answers = []\n",
    "chatgpt_semantics = []\n",
    "semantic_probs = []\n",
    "feasibilities = []\n",
    "occurances = []\n",
    "semantic_ids = []\n",
    "probs = []\n",
    "input_token_counts = []\n",
    "output_token_counts = []\n",
    "\n",
    "retrieved_scores_unc = []\n",
    "retrieved_true_scores_unc = []\n",
    "queries_unc = []\n",
    "answers_unc = []\n",
    "passages_unc = []\n",
    "chatgpt_true_scores_unc = []\n",
    "chatgpt_answers_unc = []\n",
    "occurances_unc = []\n",
    "semantic_ids_unc = []\n",
    "probs_unc = []\n",
    "        \n",
    "for idx, (element, score, retrieved) in enumerate(zip(elements, scores, retrieved_examples)):\n",
    "    if len(queries) > 1004:\n",
    "        break\n",
    "    \n",
    "    print(f'{idx}', file=open(f'chatgpt_{task}_fewshot.txt', 'a'))\n",
    "    feasible = False\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "        save_results(task)\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element, dataset=task)\n",
    "    if len(passage_id) == 0:\n",
    "        continue\n",
    "    retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "        utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "    if len(true_score) == 0:\n",
    "        continue\n",
    "    \n",
    "    prompt = utils.get_prompt_template_fewshot(\n",
    "        query, \n",
    "        passage_text[0], \n",
    "        demonstrations)\n",
    "    break\n",
    "    if chat:\n",
    "        sequences, input_token_count, output_token_count = \\\n",
    "            utils.ask_chatgpt(prompt, n_answers=30, model=\"gpt-3.5-turbo-0613\")\n",
    "    else:\n",
    "        sequences, probs = utils.ask_chatgpt(prompt)\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(sequences, prompt, scorer=scorer)\n",
    "    true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "        semantic_set_ids, semantic_probs, \n",
    "        item_occurance, answer, scorer,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    if len(true_scores) == 0:\n",
    "        retrieved_scores_unc.append(score)\n",
    "        retrieved_true_scores_unc.append(true_score)\n",
    "        queries_unc.append(query)\n",
    "        answers_unc.append(answer)\n",
    "        passages_unc.append(passage_text)\n",
    "        chatgpt_true_scores_unc.append(true_scores)\n",
    "        chatgpt_answers_unc.append(sequences)\n",
    "        occurances_unc.append(item_occurance)\n",
    "        semantic_ids_unc.append(semantic_set_ids)\n",
    "        probs_unc.append(semantic_probs)\n",
    "        input_token_counts.append(input_token_count)\n",
    "        output_token_counts.append(output_token_count)\n",
    "        continue\n",
    "    else:\n",
    "        feasible = True\n",
    "        retrieved_scores.append(score)\n",
    "        retrieved_true_scores.append(true_score)\n",
    "        queries.append(query)\n",
    "        answers.append(answer)\n",
    "        passages.append(passage_text)\n",
    "        chatgpt_true_scores.append(true_scores)\n",
    "\n",
    "        probs_tmp = []\n",
    "        answers_tmp = []\n",
    "        semantic_id_tmp = []\n",
    "        occurance_tmp = []\n",
    "        semantic_tmp = []\n",
    "        for context, s in zip(retrieved_texts, score):\n",
    "\n",
    "            prompt = utils.get_prompt_template_fewshot(\n",
    "                query, \n",
    "                context, \n",
    "                demonstrations)\n",
    "            if chat:\n",
    "                sequences, input_token_count_tmp, output_token_count_tmp = \\\n",
    "                    utils.ask_chatgpt(prompt, n_answers=30, model=\"gpt-3.5-turbo-0613\")\n",
    "            else:\n",
    "                sequences, probs = utils.ask_chatgpt(prompt, n_answers=30)\n",
    "            input_token_count += input_token_count_tmp\n",
    "            output_token_count += output_token_count_tmp\n",
    "\n",
    "            if semantic:\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.compute_semantic_clusterring(\n",
    "                        semantic_model, \n",
    "                        semantic_tokenizer,\n",
    "                        prompt,\n",
    "                        sequences,\n",
    "                    )\n",
    "            else:\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.clustering(sequences, prompt, scorer=scorer)\n",
    "\n",
    "            probs_tmp.append(semantic_probs)\n",
    "            answers_tmp.append(sequences)\n",
    "            occurance_tmp.append(item_occurance)\n",
    "            semantic_id_tmp.append(semantic_set_ids)\n",
    "\n",
    "        chatgpt_answers.append(answers_tmp)\n",
    "        feasibilities.append(feasible)\n",
    "        occurances.append(occurance_tmp)\n",
    "        semantic_ids.append(semantic_id_tmp)\n",
    "        probs.append(probs_tmp)\n",
    "        input_token_counts.append(input_token_count)\n",
    "        output_token_counts.append(output_token_count)\n",
    "print('Finished!', file=open(f'chatgpt_{task}_fewshot.txt', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the following question based on the given context; Answer the question shortly.\\n                Question: \\'\\'\\'when did sunday became the seventh day of the week\\'\\'\\'\\n                Context: \\'\\'\\'일요일 Il-yo-Il, meaning \"day of sun\". The international standard ISO 8601 for representation of dates and times, states that Sunday is the seventh and last day of the week. This method of representing dates and times unambiguously was first published in 1988. In the Judaic, some Christian, as well as in some Islamic tradition, Sunday has been considered the first day of the week. A number of languages express this position either by the name for the day or by the naming of the other days. In Hebrew it is called יום ראשון \"yom rishon\", in Arabic الأحد \"al-ahad\", in\\'\\'\\'\\n                Answer: \\'\\'\\'[\\'1988\\']\\'\\'\\'\\n                Answer the following question based on the given context; Answer the question shortly.\\n                Question: \\'\\'\\'girl from the shut up and dance video\\'\\'\\'\\n                Context: \\'\\'\\'Tour on July 24 in Foxborough, Massachusetts. Walk the Moon performed the song at the 2015 MTV Video Music Awards pre-show on August 30, 2015, where the band had played on a circular, multicolored stage prior to the show\\'s start. The band also performed the song at the 2016 NBA All-Star Game on February 13, 2016 in Toronto, Canada. The music video, a 1980s club-themed movie-style music video, was released on YouTube on October 23, 2014. It stars professional dancer Lauren Taft alongside Petricca. Credits adapted from the liner notes of \"Talking Is Hard\". Locations Personnel Shut Up and Dance\\'\\'\\'\\n                Answer: \\'\\'\\'[\\'Lauren Taft\\']\\'\\'\\'\\n                Answer the following question based on the given context; Answer the question shortly.\\n            Question: \\'\\'\\'who sings does he love me with reba\\'\\'\\'\\n            Context: \\'\\'\\'Does He Love You \"Does He Love You\" is a song written by Sandy Knox and Billy Stritch, and recorded as a duet by American country music artists Reba McEntire and Linda Davis. It was released in August 1993 as the first single from Reba\\'s album \"Greatest Hits Volume Two\". It is one of country music\\'s several songs about a love triangle. \"Does He Love You\" was written in 1982 by Billy Stritch. He recorded it with a trio in which he performed at the time, because he wanted a song that could be sung by the other two members\\'\\'\\'\\n            Answer:\\n            '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated cost for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019968\n"
     ]
    }
   ],
   "source": [
    "average_cost = np.mean([input_token_count]) / 1000 * 0.0015 + \\\n",
    "               np.mean([output_token_count]) / 1000 * 0.002\n",
    "print(average_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
