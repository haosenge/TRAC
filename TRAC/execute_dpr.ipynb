{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1, 2, 3\"\n",
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import opensource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "[140018359538048] 2023-09-25 09:37:44,767 [WARNING] datasets.builder: Found cached dataset wiki_dpr (/home/lishuo1/.cache/huggingface/datasets/wiki_dpr/psgs_w100.multiset.compressed/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "wiki = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='nq'\n",
    "dataset_dpr = tasks.RQA_dpr(task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_dpr.elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "elements = dataset_dpr.elements\n",
    "query = [element['question'] for element in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "scores, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scores to a json file\n",
    "with open(f'dpr_scores_{task}.json', 'w') as f:\n",
    "    json.dump(scores, f)\n",
    "# save retrieved examples to a json file\n",
    "with open(f'dpr_retrieved_examples_{task}.json', 'w') as f:\n",
    "    json.dump(retrieved_examples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup opensource model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, pipeline, tokenizer = opensource.setup_openmodel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "answers = []\n",
    "passages = []\n",
    "retrieved_true_scores = []\n",
    "opensource_true_scores = []\n",
    "opensource_texts = []\n",
    "opensource_answers = []\n",
    "opensource_semantics = []\n",
    "with torch.no_grad():\n",
    "    for idx, (element, score, retrieved) in enumerate(zip(elements, scores, retrieved_examples)):\n",
    "        print(idx)\n",
    "        try:\n",
    "            query, answer, passage_id, passage_title, passage_text = \\\n",
    "                utils.dataset_info(element)\n",
    "            retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "                utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "            if len(true_score) == 0:\n",
    "                continue\n",
    "            \n",
    "            prompt = utils.get_prompt_template(query, passage_text[0], task='Natural Questions')\n",
    "            try:\n",
    "                sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer, top_k=30)\n",
    "            except:\n",
    "                continue\n",
    "            generated_texts = []\n",
    "            for seq in sequences:\n",
    "                generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "            \n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(generated_texts, prompt, scorer=scorer)\n",
    "                \n",
    "            true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "                semantic_set_ids, semantic_probs, \n",
    "                item_occurance, answer, scorer,\n",
    "                threshold=0.3\n",
    "            )\n",
    "            if len(true_scores) == 0:\n",
    "                continue\n",
    "              \n",
    "            retrieved_true_scores.append(true_score)\n",
    "            queries.append(query)\n",
    "            answers.append(answer)\n",
    "            passages.append(passage_text)\n",
    "            opensource_true_scores.append(true_scores)\n",
    "            opensource_texts.append(generated_texts)\n",
    "\n",
    "            answers_tmp = []\n",
    "            semantics_tmp = []\n",
    "            for ctx_idx, (context, s) in enumerate(zip(retrieved_texts, score)):\n",
    "                prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "                sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer, top_k=30)\n",
    "                for seq in sequences:\n",
    "                    generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.clustering(generated_texts, prompt, scorer=scorer)\n",
    "                true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "                    semantic_set_ids, semantic_probs, \n",
    "                    item_occurance, answer, scorer,\n",
    "                    threshold=0.3)\n",
    "                answers_tmp.extend(generated_texts)\n",
    "                semantics_tmp.extend(semantics)\n",
    "            opensource_answers.append(answers_tmp)\n",
    "            opensource_semantics.extend(semantics_tmp)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save retrieved_true_scores to a json file\n",
    "with open(f'retrieved_true_scores_{task}.json', 'w') as f:\n",
    "    json.dump(retrieved_true_scores, f)\n",
    "# save queries to a json file\n",
    "with open(f'queries_{task}.json', 'w') as f:\n",
    "    json.dump(queries, f)\n",
    "# save answers to a json file\n",
    "with open(f'answers_{task}.json', 'w') as f:\n",
    "    json.dump(answers, f)\n",
    "# save passages to a json file\n",
    "with open(f'passages_{task}.json', 'w') as f:\n",
    "    json.dump(passages, f)\n",
    "# save opensource_true_scores to a json file\n",
    "with open(f'opensource_true_scores_{task}.json', 'w') as f:\n",
    "    json.dump(opensource_true_scores, f)\n",
    "# save opensource_texts to a json file\n",
    "with open(f'opensource_texts_{task}.json', 'w') as f:\n",
    "    json.dump(opensource_texts, f)\n",
    "# save opensource_answers to a json file\n",
    "with open(f'opensource_answers_{task}.json', 'w') as f:\n",
    "    json.dump(opensource_answers, f)\n",
    "# save opensource_semantics to a json file\n",
    "with open(f'opensource_semantics_{task}.json', 'w') as f:\n",
    "    json.dump(opensource_semantics, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_dpr.elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "test_elements = utils.split(dataset_dpr.elements, test_indices)\n",
    "cal_elements = utils.split(dataset_dpr.elements, cal_indices)\n",
    "\n",
    "test_query = [element['question'] for element in test_elements]\n",
    "cal_query = [element['question'] for element in cal_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(cal_query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "scores, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "answers = []\n",
    "retrieved_texts = []\n",
    "retrieved_scores = []\n",
    "passages = []\n",
    "retrieved_true_scores = []\n",
    "for element, score, retrieved in zip(cal_elements, scores, retrieved_examples):\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element)\n",
    "    retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "        utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "    if len(true_score) == 0:\n",
    "        continue\n",
    "    \n",
    "    retrieved_true_scores.append(true_score)\n",
    "    queries.append(query)\n",
    "    answers.append(answer)\n",
    "    passages.append(passage_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_true_scores = []\n",
    "chat = True\n",
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "with torch.no_grad():\n",
    "    for idx, element in enumerate(tqdm(cal_elements)):\n",
    "        \n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element)\n",
    "    \n",
    "        sequences, prompt = utils.ask(query, passage_text[0], chat)\n",
    "        \n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "            utils.clustering(sequences, prompt, scorer=scorer)\n",
    "            \n",
    "        true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        chatgpt_true_scores.append(true_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_thr_qa = utils.compute_threshold(chatgpt_true_scores, alpha=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup opensource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, pipeline, tokenizer = opensource.setup_openmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_true_scores = []\n",
    "with torch.no_grad():\n",
    "    for idx, element in enumerate(tqdm(cal_elements)):\n",
    "        \n",
    "        \n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element)\n",
    "    \n",
    "        prompt = utils.get_prompt_template(query, passage_text[0], task='Natural Questions')\n",
    "        try:\n",
    "            sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer, top_k=30)\n",
    "        except:\n",
    "            continue\n",
    "        generated_texts = []\n",
    "        for seq in sequences:\n",
    "            generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "        sequences = generated_texts\n",
    "        \n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "            utils.clustering(sequences, prompt, scorer=scorer)\n",
    "            \n",
    "        true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        opensource_true_scores.append(true_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retrieved_threshold = utils.compute_threshold(retrieved_true_scores, alpha=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_thr_qa = utils.compute_threshold(opensource_true_scores, alpha=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(test_query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "test_scores, test_retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid(element, score, retrieved, thr_qa, chat=True):\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element)\n",
    "    retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "        utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "\n",
    "    if len(true_score) == 0:\n",
    "        return False, True\n",
    "\n",
    "    sequences, prompt = utils.ask(query, passage_text[0], chat, task='Natural Questions')\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(sequences, prompt, scorer=scorer)\n",
    "    true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "        semantic_set_ids, semantic_probs, \n",
    "        item_occurance, answer, scorer,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    if len(true_scores) == 0:\n",
    "        return False, True\n",
    "    elif np.sum(np.array(true_scores) >= chatgpt_thr_qa) == 0:\n",
    "        chatgpt_covered.append(False)\n",
    "        return True, False\n",
    "    else:\n",
    "        return True, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid_opensource(element, score, retrieved, thr_qa):\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element)\n",
    "    retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "        utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "\n",
    "    if len(true_score) == 0:\n",
    "        return False, True\n",
    "\n",
    "#     sequences, prompt = utils.ask(query, passage_text[0], chat, task='Natural Questions')\n",
    "    prompt = utils.get_prompt_template(query, passage_text[0], task='Natural Questions')\n",
    "    sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer, top_k=30)\n",
    "    generated_texts = []\n",
    "    for seq in sequences:\n",
    "        generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "    sequences = generated_texts\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(sequences, prompt, scorer=scorer)\n",
    "    true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "        semantic_set_ids, semantic_probs, \n",
    "        item_occurance, answer, scorer,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    if len(true_scores) == 0:\n",
    "        return False, True\n",
    "    elif np.sum(np.array(true_scores) >= thr_qa) == 0:\n",
    "        return True, False\n",
    "    else:\n",
    "        return True, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_covered = []\n",
    "chatgpt_sizes = []\n",
    "queries = []\n",
    "with torch.no_grad():\n",
    "    for idx, (element, score, retrieved) in enumerate(zip(test_elements, test_scores, test_retrieved_examples)):\n",
    "        print(idx)\n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element)\n",
    "        retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "            utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "        \n",
    "        if idx > 100:\n",
    "            break\n",
    "        \n",
    "        valid, covered = check_valid(element, score, retrieved, chatgpt_thr_qa)\n",
    "        if not valid:\n",
    "            continue\n",
    "        elif not covered:\n",
    "            print(False)\n",
    "            chatgpt_covered.append(False)\n",
    "            continue\n",
    "        \n",
    "        cover = False\n",
    "        tmp = []\n",
    "        query_count = 0\n",
    "        for ctx_idx, (context, s) in enumerate(zip(contexts, score)):\n",
    "            if s < retrieved_threshold:\n",
    "                continue\n",
    "            query_count += 1\n",
    "            sequences, prompt = utils.ask(query, context, chat, task='Natural Questions')\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(sequences, prompt, scorer=scorer)\n",
    "            true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "                semantic_set_ids, semantic_probs, \n",
    "                item_occurance, answer, scorer,\n",
    "                threshold=0.3)\n",
    "\n",
    "            tmp.extend(semantics)\n",
    "            if len(true_scores) >= 1:\n",
    "                cover = True\n",
    "                break\n",
    "        print(cover)\n",
    "        chatgpt_covered.append(cover)\n",
    "        chatgpt_sizes.append(len(tmp))\n",
    "        queries.append(query_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coverage', np.mean(chatgpt_covered))\n",
    "print('average size', np.mean(chatgpt_sizes))\n",
    "print('average query count', np.mean(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_semantic_sizes = []\n",
    "for semantic_meaning in semantics:\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(semantic_meaning, prompt, scorer=scorer)\n",
    "    chatgpt_semantic_sizes.append(len(semantic_set_ids.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average semantic size', np.mean(chatgpt_semantic_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_covered = []\n",
    "opensource_sizes = []\n",
    "queries = []\n",
    "with torch.no_grad():\n",
    "    for idx, (element, score, retrieved) in enumerate(zip(test_elements, test_scores, test_retrieved_examples)):\n",
    "        print(idx)\n",
    "        try:\n",
    "            query, answer, passage_id, passage_title, passage_text = \\\n",
    "                utils.dataset_info(element)\n",
    "            retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "                utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "\n",
    "            if idx > 100:\n",
    "                break\n",
    "\n",
    "            valid, covered = check_valid_opensource(element, score, retrieved, opensource_thr_qa)\n",
    "            if not valid:\n",
    "                continue\n",
    "            elif not covered:\n",
    "                print(False)\n",
    "                opensource_covered.append(False)\n",
    "                continue\n",
    "\n",
    "            cover = False\n",
    "            tmp = []\n",
    "            query_count = 0\n",
    "            for ctx_idx, (context, s) in enumerate(zip(retrieved_texts, score)):\n",
    "                if s < retrieved_threshold:\n",
    "                    continue\n",
    "                query_count += 1\n",
    "                prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "                sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer, top_k=30)\n",
    "                for seq in sequences:\n",
    "                    generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "                sequences = generated_texts\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.clustering(sequences, prompt, scorer=scorer)\n",
    "                true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "                    semantic_set_ids, semantic_probs, \n",
    "                    item_occurance, answer, scorer,\n",
    "                    threshold=0.3)\n",
    "\n",
    "                tmp.extend(semantics)\n",
    "                if len(true_scores) >= 1:\n",
    "                    cover = True\n",
    "                    break\n",
    "            print(cover)\n",
    "            opensource_covered.append(cover)\n",
    "            opensource_sizes.append(len(tmp))\n",
    "            queries.append(query_count)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(opensource_covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coverage', np.mean(opensource_covered))\n",
    "print('average size', np.mean(opensource_sizes))\n",
    "print('average query count', np.mean(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_semantic_sizes = []\n",
    "for semantic_meaning in semantics:\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(semantic_meaning, prompt, scorer=scorer)\n",
    "    opensource_semantic_sizes.append(len(semantic_set_ids.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average semantic size', np.mean(opensource_semantic_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
