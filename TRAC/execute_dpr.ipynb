{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 2, 3, 7\"\n",
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "from kilt.retrievers import DPR_connector\n",
    "import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", device_map='cuda')\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", device_map='cuda')\n",
    "wiki = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RQA_dpr:\n",
    "    def __init__(self, task='nq') -> None:\n",
    "        self.task = task\n",
    "        self.query_data, self.validated_data, self.elements = self.load_dataset()\n",
    "    \n",
    "    def load_dataset(self) -> None:\n",
    "        if self.task == 'nq':\n",
    "            with open(\"data/biencoder-nq-dev.json\", \"r\") as fin:\n",
    "                nq_dpr = json.load(fin)\n",
    "        elif self.task == 'trivia':\n",
    "            with open(\"data/biencoder-qas-dev.json\", \"r\") as fin:\n",
    "                nq_dpr = json.load(fin)\n",
    "        \n",
    "        elements = []\n",
    "        query_data = []\n",
    "        validated_data = {}\n",
    "        for idx, record in enumerate(nq_dpr):\n",
    "            elements.append(record)\n",
    "            validated_data[idx] = record\n",
    "            query_data.append( \n",
    "                record['question']\n",
    "            )\n",
    "        return query_data, validated_data, elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dpr = RQA_dpr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.arange(len(dataset_dpr.elements))\n",
    "indices = np.arange(1000)\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "test_elements = utils.split(dataset_dpr.elements, test_indices)\n",
    "cal_elements = utils.split(dataset_dpr.elements, cal_indices)\n",
    "\n",
    "test_query = [element['question'] for element in test_elements]\n",
    "cal_query = [element['question'] for element in cal_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(cal_query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "scores, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "answers = []\n",
    "retrieved_texts = []\n",
    "retrieved_scores = []\n",
    "passages = []\n",
    "retrieved_true_scores = []\n",
    "for element, score, retrieved in zip(cal_elements, scores, retrieved_examples):\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element)\n",
    "    retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "        utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "    if len(true_score) == 0:\n",
    "        continue\n",
    "    \n",
    "    retrieved_true_scores.append(true_score)\n",
    "    queries.append(query)\n",
    "    answers.append(answer)\n",
    "    passages.append(passage_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_true_scores = []\n",
    "chat = True\n",
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "with torch.no_grad():\n",
    "    for idx, element in enumerate(tqdm(cal_elements)):\n",
    "        \n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element)\n",
    "    \n",
    "        sequences, prompt = utils.ask(query, passage_text[0], chat)\n",
    "        \n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "            utils.clustering(sequences, prompt, scorer=scorer)\n",
    "            \n",
    "        true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        chatgpt_true_scores.append(true_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant threshold: 64.70359802246094\n",
      "Most relevant coverage: 0.963855421686747\n"
     ]
    }
   ],
   "source": [
    "retrieved_threshold = utils.compute_threshold(retrieved_true_scores, alpha=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant threshold: 0.16666666666666666\n",
      "Most relevant coverage: 0.9589521452145214\n"
     ]
    }
   ],
   "source": [
    "chatgpt_thr_qa = utils.compute_threshold(chatgpt_true_scores, alpha=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(test_query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "test_scores, test_retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/lishuo1/retriever_uncertainty/TRAC/utils.py'>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid(element, score, retrieved, chat=True):\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element)\n",
    "    retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "        utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "\n",
    "    if len(true_score) == 0:\n",
    "        return False, True\n",
    "\n",
    "    sequences, prompt = utils.ask(query, passage_text[0], chat, task='Natural Questions')\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(sequences, prompt, scorer=scorer)\n",
    "    true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "        semantic_set_ids, semantic_probs, \n",
    "        item_occurance, answer, scorer,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    if len(true_scores) == 0:\n",
    "        return False, True\n",
    "    elif np.sum(np.array(true_scores) >= chatgpt_thr_qa) == 0:\n",
    "        chatgpt_covered.append(False)\n",
    "        return True, False\n",
    "    else:\n",
    "        return True, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "chatgpt_covered = []\n",
    "chatgpt_sizes = []\n",
    "queries = []\n",
    "with torch.no_grad():\n",
    "    for idx, (element, score, retrieved) in enumerate(zip(test_elements, test_scores, test_retrieved_examples)):\n",
    "        print(idx)\n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element)\n",
    "        retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "            utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "        \n",
    "        if idx > 100:\n",
    "            break\n",
    "        \n",
    "        valid, covered = check_valid(element, score, retrieved, chatgpt_thr_qa)\n",
    "        if not valid:\n",
    "            continue\n",
    "        elif not covered:\n",
    "            print(False)\n",
    "            chatgpt_covered.append(False)\n",
    "            continue\n",
    "        \n",
    "        cover = False\n",
    "        tmp = []\n",
    "        query_count = 0\n",
    "        for ctx_idx, (context, s) in enumerate(zip(contexts, score)):\n",
    "            if s < retrieved_threshold:\n",
    "                continue\n",
    "            query_count += 1\n",
    "            sequences, prompt = utils.ask(query, context, chat, task='Natural Questions')\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(sequences, prompt, scorer=scorer)\n",
    "            true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "                semantic_set_ids, semantic_probs, \n",
    "                item_occurance, answer, scorer,\n",
    "                threshold=0.3)\n",
    "\n",
    "            tmp.extend(semantics)\n",
    "            if len(true_scores) >= 1:\n",
    "                cover = True\n",
    "                break\n",
    "        print(cover)\n",
    "        chatgpt_covered.append(cover)\n",
    "        chatgpt_sizes.append(len(tmp))\n",
    "        queries.append(query_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coverage 0.8936170212765957\n",
      "average size 54.651162790697676\n",
      "average query count 1.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "print('coverage', np.mean(chatgpt_covered))\n",
    "print('average size', np.mean(chatgpt_sizes))\n",
    "print('average query count', np.mean(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_semantic_sizes = []\n",
    "for semantic_meaning in semantics:\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(semantic_meaning, prompt, scorer=scorer)\n",
    "    chatgpt_semantic_sizes.append(len(semantic_set_ids.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average semantic size 6.4\n"
     ]
    }
   ],
   "source": [
    "print('average semantic size', np.mean(chatgpt_semantic_sizes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
