{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1, 2, 3\"\n",
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "from kilt.retrievers import DPR_connector\n",
    "import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "[140080826257792] 2023-09-20 15:05:18,674 [WARNING] datasets.builder: Found cached dataset wiki_dpr (/home/lishuo1/.cache/huggingface/datasets/wiki_dpr/psgs_w100.multiset.compressed/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", device_map='cuda')\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", device_map='cuda')\n",
    "wiki = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RQA_dpr:\n",
    "    def __init__(self, task='nq') -> None:\n",
    "        self.task = task\n",
    "        self.query_data, self.validated_data, self.elements = self.load_dataset()\n",
    "    \n",
    "    def load_dataset(self) -> None:\n",
    "        if self.task == 'nq':\n",
    "            with open(\"data/biencoder-nq-dev.json\", \"r\") as fin:\n",
    "                nq_dpr = json.load(fin)\n",
    "        elif self.task == 'trivia':\n",
    "            with open(\"data/biencoder-qas-dev.json\", \"r\") as fin:\n",
    "                nq_dpr = json.load(fin)\n",
    "        \n",
    "        elements = []\n",
    "        query_data = []\n",
    "        validated_data = {}\n",
    "        for idx, record in enumerate(nq_dpr):\n",
    "            elements.append(record)\n",
    "            validated_data[idx] = record\n",
    "            query_data.append( \n",
    "                record['question']\n",
    "            )\n",
    "        return query_data, validated_data, elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dpr = RQA_dpr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.arange(len(dataset_dpr.elements))\n",
    "indices = np.arange(1000)\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "test_elements = utils.split(dataset_dpr.elements, test_indices)\n",
    "cal_elements = utils.split(dataset_dpr.elements, cal_indices)\n",
    "\n",
    "test_query = [element['question'] for element in test_elements]\n",
    "cal_query = [element['question'] for element in cal_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(cal_query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "scores, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "answers = []\n",
    "retrieved_texts = []\n",
    "retrieved_scores = []\n",
    "passages = []\n",
    "retrieved_true_scores = []\n",
    "for element, score, retrieved in zip(cal_elements, scores, retrieved_examples):\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element)\n",
    "    retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "        utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "    if len(true_score) == 0:\n",
    "        continue\n",
    "    \n",
    "    retrieved_true_scores.append(true_score)\n",
    "    queries.append(query)\n",
    "    answers.append(answer)\n",
    "    passages.append(passage_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_true_scores = []\n",
    "chat = True\n",
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "with torch.no_grad():\n",
    "    for idx, element in enumerate(tqdm(cal_elements)):\n",
    "        \n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element)\n",
    "    \n",
    "        sequences, prompt = utils.ask(query, passage_text[0], chat)\n",
    "        \n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "            utils.clustering(sequences, prompt, scorer=scorer)\n",
    "            \n",
    "        true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        chatgpt_true_scores.append(true_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'opensource' from '/home/lishuo1/retriever_uncertainty/TRAC/opensource.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import opensource\n",
    "importlib.reload(opensource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[140080826257792] 2023-09-20 15:09:37,376 [WARNING] accelerate.utils.modeling: The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.33s/it]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "model, pipeline, tokenizer = opensource.setup_openmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_opensource(query, pipeline, tokenizer, task=\"Natural Questions\"):\n",
    "    prompt = utils.get_prompt_template(query, passage_text[0], task=task)\n",
    "    try:\n",
    "        sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer)\n",
    "    except:\n",
    "        continue\n",
    "    generated_texts = []\n",
    "    for seq in sequences:\n",
    "        generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "    return generated_texts, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 168/500 [41:51<1:30:56, 16.44s/it]"
     ]
    }
   ],
   "source": [
    "opensource_true_scores = []\n",
    "with torch.no_grad():\n",
    "    for idx, element in enumerate(tqdm(cal_elements)):\n",
    "        \n",
    "        \n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element)\n",
    "    \n",
    "        prompt = utils.get_prompt_template(query, passage_text[0], task='Natural Questions')\n",
    "        try:\n",
    "            sequences = opensource.ask_openmodel(prompt, pipeline, tokenizer)\n",
    "        except:\n",
    "            continue\n",
    "        generated_texts = []\n",
    "        for seq in sequences:\n",
    "            generated_texts.append(seq['generated_text'][len(prompt):].strip())\n",
    "        sequences = generated_texts\n",
    "        \n",
    "        semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "            utils.clustering(sequences, prompt, scorer=scorer)\n",
    "            \n",
    "        true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "            semantic_set_ids, semantic_probs, \n",
    "            item_occurance, answer, scorer,\n",
    "            threshold=0.3\n",
    "        )\n",
    "        opensource_true_scores.append(true_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant threshold: 65.75518035888672\n",
      "Most relevant coverage: 0.9202453987730062\n"
     ]
    }
   ],
   "source": [
    "retrieved_threshold = utils.compute_threshold(retrieved_true_scores, alpha=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_thr_qa = utils.compute_threshold(chatgpt_true_scores, alpha=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_thr_qa = utils.compute_threshold(opensource_true_scores, alpha=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(test_query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "test_scores, test_retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/lishuo1/retriever_uncertainty/TRAC/utils.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid(element, score, retrieved, chat=True):\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element)\n",
    "    retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "        utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "\n",
    "    if len(true_score) == 0:\n",
    "        return False, True\n",
    "\n",
    "    sequences, prompt = utils.ask(query, passage_text[0], chat, task='Natural Questions')\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(sequences, prompt, scorer=scorer)\n",
    "    true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "        semantic_set_ids, semantic_probs, \n",
    "        item_occurance, answer, scorer,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    if len(true_scores) == 0:\n",
    "        return False, True\n",
    "    elif np.sum(np.array(true_scores) >= chatgpt_thr_qa) == 0:\n",
    "        chatgpt_covered.append(False)\n",
    "        return True, False\n",
    "    else:\n",
    "        return True, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_covered = []\n",
    "chatgpt_sizes = []\n",
    "queries = []\n",
    "with torch.no_grad():\n",
    "    for idx, (element, score, retrieved) in enumerate(zip(test_elements, test_scores, test_retrieved_examples)):\n",
    "        print(idx)\n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element)\n",
    "        retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "            utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "        \n",
    "        if idx > 100:\n",
    "            break\n",
    "        \n",
    "        valid, covered = check_valid(element, score, retrieved, chatgpt_thr_qa)\n",
    "        if not valid:\n",
    "            continue\n",
    "        elif not covered:\n",
    "            print(False)\n",
    "            chatgpt_covered.append(False)\n",
    "            continue\n",
    "        \n",
    "        cover = False\n",
    "        tmp = []\n",
    "        query_count = 0\n",
    "        for ctx_idx, (context, s) in enumerate(zip(contexts, score)):\n",
    "            if s < retrieved_threshold:\n",
    "                continue\n",
    "            query_count += 1\n",
    "            sequences, prompt = utils.ask(query, context, chat, task='Natural Questions')\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(sequences, prompt, scorer=scorer)\n",
    "            true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "                semantic_set_ids, semantic_probs, \n",
    "                item_occurance, answer, scorer,\n",
    "                threshold=0.3)\n",
    "\n",
    "            tmp.extend(semantics)\n",
    "            if len(true_scores) >= 1:\n",
    "                cover = True\n",
    "                break\n",
    "        print(cover)\n",
    "        chatgpt_covered.append(cover)\n",
    "        chatgpt_sizes.append(len(tmp))\n",
    "        queries.append(query_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coverage', np.mean(chatgpt_covered))\n",
    "print('average size', np.mean(chatgpt_sizes))\n",
    "print('average query count', np.mean(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_semantic_sizes = []\n",
    "for semantic_meaning in semantics:\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(semantic_meaning, prompt, scorer=scorer)\n",
    "    chatgpt_semantic_sizes.append(len(semantic_set_ids.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average semantic size', np.mean(chatgpt_semantic_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_covered = []\n",
    "opensource_sizes = []\n",
    "queries = []\n",
    "with torch.no_grad():\n",
    "    for idx, (element, score, retrieved) in enumerate(zip(test_elements, test_scores, test_retrieved_examples)):\n",
    "        print(idx)\n",
    "        query, answer, passage_id, passage_title, passage_text = \\\n",
    "            utils.dataset_info(element)\n",
    "        retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "            utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "        \n",
    "        if idx > 100:\n",
    "            break\n",
    "        \n",
    "        valid, covered = check_valid(element, score, retrieved, chatgpt_thr_qa)\n",
    "        if not valid:\n",
    "            continue\n",
    "        elif not covered:\n",
    "            print(False)\n",
    "            chatgpt_covered.append(False)\n",
    "            continue\n",
    "        \n",
    "        cover = False\n",
    "        tmp = []\n",
    "        query_count = 0\n",
    "        for ctx_idx, (context, s) in enumerate(zip(contexts, score)):\n",
    "            if s < retrieved_threshold:\n",
    "                continue\n",
    "            query_count += 1\n",
    "            sequences, prompt = ask_opensource(query, context, chat, task='Natural Questions')\n",
    "            semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                utils.clustering(sequences, prompt, scorer=scorer)\n",
    "            true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "                semantic_set_ids, semantic_probs, \n",
    "                item_occurance, answer, scorer,\n",
    "                threshold=0.3)\n",
    "\n",
    "            tmp.extend(semantics)\n",
    "            if len(true_scores) >= 1:\n",
    "                cover = True\n",
    "                break\n",
    "        print(cover)\n",
    "        opensource_covered.append(cover)\n",
    "        opensource_sizes.append(len(tmp))\n",
    "        queries.append(query_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coverage', np.mean(opensource_covered))\n",
    "print('average size', np.mean(opensource_sizes))\n",
    "print('average query count', np.mean(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensource_semantic_sizes = []\n",
    "for semantic_meaning in semantics:\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(semantic_meaning, prompt, scorer=scorer)\n",
    "    chatgpt_semantic_sizes.append(len(semantic_set_ids.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average semantic size', np.mean(opensource_semantic_sizes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
