{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1, 2, 3\"\n",
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Found cached dataset wiki_dpr (/home/lishuo1/.cache/huggingface/datasets/wiki_dpr/psgs_w100.multiset.compressed/0.0.0/74d4bff38a7c18a9498fafef864a8ba7129e27cb8d71b22f5e14d84cb17edd54)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device_map='cuda')\n",
    "wiki = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='nq'\n",
    "dataset_dpr = tasks.RQA_dpr(task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset_dpr.elements))\n",
    "random.shuffle(indices)\n",
    "cal_indices = indices[:int(len(indices) * 0.5)]\n",
    "test_indices = indices[int(len(indices) * 0.5):]\n",
    "\n",
    "elements = dataset_dpr.elements\n",
    "query = [element['question'] for element in elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = q_encoder(**q_tokenizer(query, return_tensors=\"pt\", padding=True))\n",
    "question_embedding = question_embedding[0].numpy()\n",
    "scores, retrieved_examples = wiki.get_nearest_examples_batch('embeddings', question_embedding, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "#         print('Done writing list into a binary file')\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "def save_results(task):\n",
    "    # save retrieved_scores to a pickle file\n",
    "    write_list(retrieved_scores, f'chatgpt_retrieved_scores_{task}.p')\n",
    "    # save retrieved_true_scores to a pickle file\n",
    "    write_list(retrieved_true_scores, f'chatgpt_retrieved_true_scores_{task}.p')\n",
    "    # save queries to a pickle file\n",
    "    write_list(queries, f'chatgpt_queries_{task}.p')\n",
    "    # save answers to a pickle file\n",
    "    write_list(answers, f'chatgpt_true_answers_{task}.p')\n",
    "    # save passages to a pickle file\n",
    "    write_list(passages, f'chatgpt_passages_{task}.p')\n",
    "    # save chatgpt_true_scores to a pickle file\n",
    "    write_list(chatgpt_true_scores, f'chatgpt_true_scores_{task}.p')\n",
    "    # save chatgpt_texts to a pickle file\n",
    "#     write_list(chatgpt_texts, f'chatgpt_texts_{task}.p')\n",
    "    # save chatgpt_answers to a pickle file\n",
    "    write_list(chatgpt_answers, f'chatgpt_answers_{task}.p')\n",
    "    # save chatgpt_semantics to a picle file\n",
    "    write_list(chatgpt_semantics, f'chatgpt_semantics_{task}.p')\n",
    "    # save feasibilities to a pickle file\n",
    "    write_list(feasibilities, f'chatgpt_feasibilities_{task}.p')\n",
    "    # save occurances to a pickle file\n",
    "    write_list(occurances, f'chatgpt_occurances_{task}.p')\n",
    "    # save semantic_ids to a pickle file\n",
    "    write_list(semantic_ids, f'chatgpt_semantic_ids_{task}.p')\n",
    "    # save probs to a picle file\n",
    "    write_list(probs, f'chatgpt_probs_{task}.p')\n",
    "    \n",
    "    write_list(retrieved_scores_unc, f'chatgpt_retrieved_scores_unc_{task}.p')\n",
    "    write_list(retrieved_true_scores_unc, f'chatgpt_retrieved_true_scores_unc_{task}.p')\n",
    "    write_list(queries_unc, f'chatgpt_queries_unc_{task}.p')\n",
    "    write_list(answers_unc, f'chatgpt_answers_unc_{task}.p')\n",
    "    write_list(passages_unc, f'chatgpt_passages_unc_{task}.p')\n",
    "    write_list(chatgpt_true_scores_unc, f'chatgpt_true_scores_unc_{task}.p')\n",
    "    write_list(chatgpt_answers_unc, f'chatgpt_answers_unc_{task}.p')\n",
    "    write_list(occurances_unc, f'chatgpt_occurances_unc_{task}.p')\n",
    "    write_list(semantic_ids_unc, f'chatgpt_semantic_ids_unc_{task}.p')\n",
    "    write_list(probs_unc, f'chatgpt_probs_unc_{task}.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(task):\n",
    "    retrieved_scores = read_list(f'chatgpt_retrieved_scores_{task}.p')\n",
    "    retrieved_true_scores = read_list(f'chatgpt_retrieved_true_scores_{task}.p')\n",
    "    queries = read_list(f'chatgpt_queries_{task}.p')\n",
    "    answers = read_list(f'chatgpt_answers_{task}.p')\n",
    "    chatgpt_true_scores = read_list(f'chatgpt_true_scores_{task}.p')\n",
    "    chatgpt_answers = read_list(f'chatgpt_answers_{task}.p')\n",
    "    chatgpt_passages = read_list(f'chatgpt_passages_{task}.p')\n",
    "    chatgpt_semantics = read_list(f'chatgpt_semantics_{task}.p')\n",
    "    chatgpt_occurances = read_list(f'chatgpt_occurances_{task}.p')\n",
    "    chatgpt_semantic_ids = read_list(f'chatgpt_semantic_ids_{task}.p')\n",
    "    chatgpt_probs = read_list(f'chatgpt_probs_{task}.p')\n",
    "    \n",
    "    retrieved_scores_unc = read_list(f'chatgpt_retrieved_scores_unc_{task}.p')\n",
    "    retrieved_true_scores_unc = read_list(f'chatgpt_retrieved_true_scores_unc_{task}.p')\n",
    "    queries_unc = read_list(f'chatgpt_queries_unc_{task}.p')\n",
    "    answers_unc = read_list(f'chatgpt_answers_unc_{task}.p')\n",
    "    passages_unc = read_list(f'chatgpt_passages_unc_{task}.p')\n",
    "    chatgpt_true_scores_unc = read_list(f'chatgpt_true_scores_unc_{task}.p')\n",
    "    chatgpt_answers_unc = read_list(f'chatgpt_answers_unc_{task}.p')\n",
    "    chatgpt_occurances_unc = read_list(f'chatgpt_occurances_unc_{task}.p')\n",
    "    chatgpt_semantic_ids_unc = read_list(f'chatgpt_semantic_ids_unc_{task}.p')\n",
    "    chatgpt_probs_unc = read_list(f'chatgpt_probs_unc_{task}.p')\n",
    "    \n",
    "    return retrieved_scores, retrieved_true_scores, queries, answers, chatgpt_true_scores, chatgpt_answers, chatgpt_passages, chatgpt_semantics, chatgpt_occurances, chatgpt_semantic_ids, chatgpt_probs, retrieved_scores_unc, retrieved_true_scores_unc, queries_unc, answers_unc, passages_unc, chatgpt_true_scores_unc, chatgpt_answers_unc, chatgpt_occurances_unc, chatgpt_semantic_ids_unc, chatgpt_probs_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chat = True\n",
    "semantic = False\n",
    "queries = []\n",
    "answers = []\n",
    "passages = []\n",
    "retrieved_scores = []\n",
    "retrieved_true_scores = []\n",
    "chatgpt_true_scores = []\n",
    "chatgpt_texts = []\n",
    "chatgpt_answers = []\n",
    "chatgpt_semantics = []\n",
    "semantic_probs = []\n",
    "feasibilities = []\n",
    "occurances = []\n",
    "semantic_ids = []\n",
    "probs = []\n",
    "input_token_counts = []\n",
    "output_token_counts = []\n",
    "\n",
    "retrieved_scores_unc = []\n",
    "retrieved_true_scores_unc = []\n",
    "queries_unc = []\n",
    "answers_unc = []\n",
    "passages_unc = []\n",
    "chatgpt_true_scores_unc = []\n",
    "chatgpt_answers_unc = []\n",
    "occurances_unc = []\n",
    "semantic_ids_unc = []\n",
    "probs_unc = []\n",
    "        \n",
    "for idx, (element, score, retrieved) in enumerate(zip(elements, scores, retrieved_examples)):\n",
    "    if len(queries) > 1005:\n",
    "        break\n",
    "    \n",
    "    print(f'{idx}', file=open(f'chatgpt_{task}.txt', 'a'))\n",
    "    feasible = False\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)\n",
    "        save_results(task)\n",
    "    query, answer, passage_id, passage_title, passage_text = \\\n",
    "        utils.dataset_info(element, dataset=task)\n",
    "    if len(passage_id) == 0:\n",
    "        continue\n",
    "    retrieved_ids, retrieved_texts, retrieved_title, true_score = \\\n",
    "        utils.retrieved_info(score, retrieved, passage_id[0])\n",
    "    if len(true_score) == 0:\n",
    "        continue\n",
    "    \n",
    "    prompt = utils.get_prompt_template(query, passage_text[0], task='Natural Questions')\n",
    "    if chat:\n",
    "        sequences, input_token_count, output_token_count = \\\n",
    "            utils.ask_chatgpt(prompt, n_answers=30, model=\"gpt-3.5-turbo-0613\")\n",
    "    else:\n",
    "        sequences, probs = utils.ask_chatgpt(prompt)\n",
    "    semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "        utils.clustering(sequences, prompt, scorer=scorer)\n",
    "    true_scores, matched_answer, semantics = utils.processing_answers(\n",
    "        semantic_set_ids, semantic_probs, \n",
    "        item_occurance, answer, scorer,\n",
    "        threshold=0.3\n",
    "    )\n",
    "    if len(true_scores) == 0:\n",
    "        retrieved_scores_unc.append(score)\n",
    "        retrieved_true_scores_unc.append(true_score)\n",
    "        queries_unc.append(query)\n",
    "        answers_unc.append(answer)\n",
    "        passages_unc.append(passage_text)\n",
    "        chatgpt_true_scores_unc.append(true_scores)\n",
    "        chatgpt_answers_unc.append(sequences)\n",
    "        occurances_unc.append(item_occurance)\n",
    "        semantic_ids_unc.append(semantic_set_ids)\n",
    "        probs_unc.append(semantic_probs)\n",
    "        input_token_counts.append(input_token_count)\n",
    "        output_token_counts.append(output_token_count)\n",
    "        continue\n",
    "    else:\n",
    "        feasible = True\n",
    "        retrieved_scores.append(score)\n",
    "        retrieved_true_scores.append(true_score)\n",
    "        queries.append(query)\n",
    "        answers.append(answer)\n",
    "        passages.append(passage_text)\n",
    "        chatgpt_true_scores.append(true_scores)\n",
    "\n",
    "        probs_tmp = []\n",
    "        answers_tmp = []\n",
    "        semantic_id_tmp = []\n",
    "        occurance_tmp = []\n",
    "        semantic_tmp = []\n",
    "        for context, s in zip(retrieved_texts, score):\n",
    "\n",
    "            prompt = utils.get_prompt_template(query, context, task='Natural Questions')\n",
    "            if chat:\n",
    "                sequences, input_token_count_tmp, output_token_count_tmp = \\\n",
    "                    utils.ask_chatgpt(prompt, n_answers=30, model=\"gpt-3.5-turbo-0613\")\n",
    "            else:\n",
    "                sequences, probs = utils.ask_chatgpt(prompt, n_answers=30)\n",
    "            input_token_count += input_token_count_tmp\n",
    "            output_token_count += output_token_count_tmp\n",
    "\n",
    "            if semantic:\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.compute_semantic_clusterring(\n",
    "                        semantic_model, \n",
    "                        semantic_tokenizer,\n",
    "                        prompt,\n",
    "                        sequences,\n",
    "                    )\n",
    "            else:\n",
    "                semantic_set_ids, semantic_probs, item_occurance = \\\n",
    "                    utils.clustering(sequences, prompt, scorer=scorer)\n",
    "\n",
    "            probs_tmp.append(semantic_probs)\n",
    "            answers_tmp.append(sequences)\n",
    "            occurance_tmp.append(item_occurance)\n",
    "            semantic_id_tmp.append(semantic_set_ids)\n",
    "\n",
    "        chatgpt_answers.append(answers_tmp)\n",
    "        feasibilities.append(feasible)\n",
    "        occurances.append(occurance_tmp)\n",
    "        semantic_ids.append(semantic_id_tmp)\n",
    "        probs.append(probs_tmp)\n",
    "        input_token_counts.append(input_token_count)\n",
    "        output_token_counts.append(output_token_count)\n",
    "print('Finished!', file=open(f'chatgpt_{task}.txt', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated cost for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lishuo1/LLM_UQ/openai-quickstart-python/venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/lishuo1/LLM_UQ/openai-quickstart-python/venv/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "average_cost = np.mean(input_token_counts) / 1000 * 0.0015 + \\\n",
    "               np.mean(output_token_counts) / 1000 * 0.002\n",
    "print(average_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
