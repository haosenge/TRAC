{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lishuo1/anaconda3/envs/kilt37/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\"\n",
    "from kilt import retrieval\n",
    "from kilt import kilt_utils as utils\n",
    "import tasks\n",
    "# from kilt.retrievers import DPR_connector\n",
    "import utils\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import opensource\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list\n",
    "\n",
    "def read_results(task):\n",
    "    retrieved_scores = read_list(f'uncertain_retrieved_scores_{task}.p')\n",
    "    retrieved_true_scores = read_list(f'uncertain_retrieved_true_scores_{task}.p')\n",
    "    queries = read_list(f'uncertain_queries_{task}.p')\n",
    "    answers = read_list(f'uncertain_answers_{task}.p')\n",
    "    opensource_true_scores = read_list(f'uncertain_opensource_true_scores_{task}.p')\n",
    "    opensource_answers = read_list(f'uncertain_opensource_answers_{task}.p')\n",
    "    opensource_semantics = read_list(f'uncertain_opensource_semantics_{task}.p')\n",
    "    opensource_occurances = read_list(f'uncertain_occurances_{task}.p')\n",
    "    opensource_semantic_ids = read_list(f'uncertain_semantic_ids_{task}.p')\n",
    "    opensource_probs = read_list(f'uncertain_probs_{task}.p')\n",
    "    \n",
    "    return retrieved_scores, retrieved_true_scores, \\\n",
    "           queries, answers, \\\n",
    "           opensource_true_scores, opensource_answers, \\\n",
    "           opensource_occurances, opensource_semantic_ids, \\\n",
    "           opensource_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'trivia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic = False\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)\n",
    "if semantic:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    # setup semantic model\n",
    "    semantic_tokenizer = \\\n",
    "        AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "    semantic_model = \\\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-large-mnli\"\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_scores, retrieved_true_scores, queries, answers, opensource_true_scores, opensource_answers, opensource_occurances, opensource_semantic_ids, opensource_probs = \\\n",
    "read_results(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(retrieved_true_scores)-1)\n",
    "random.shuffle(indices)\n",
    "cal_first_indices = indices[:int(len(indices) * 0.3)]\n",
    "cal_second_indices = indices[int(len(indices) * 0.3) : int(len(indices) * 0.6)]\n",
    "test_indices = indices[int(len(indices) * 0.6):]\n",
    "\n",
    "cal_first_retrieved_true_scores = utils.split(retrieved_true_scores, cal_first_indices)\n",
    "cal_second_retrieved_true_scores = utils.split(retrieved_true_scores, cal_second_indices)\n",
    "test_retrieved_true_scores = utils.split(retrieved_true_scores, test_indices)\n",
    "cal_first_opensource_true_scores = utils.split(opensource_true_scores, cal_first_indices)\n",
    "cal_second_opensource_true_scores = utils.split(opensource_true_scores, cal_second_indices)\n",
    "test_opensource_true_scores = utils.split(opensource_true_scores, test_indices)\n",
    "cal_first_retrieved_scores = utils.split(retrieved_scores, cal_first_indices)\n",
    "cal_second_retrieved_scores = utils.split(retrieved_scores, cal_second_indices)\n",
    "test_retrieved_scores = utils.split(retrieved_scores, test_indices)\n",
    "cal_first_opensource_occurances = utils.split(opensource_occurances, cal_first_indices)\n",
    "cal_second_opensource_occurances = utils.split(opensource_occurances, cal_second_indices)\n",
    "test_opensource_occurances = utils.split(opensource_occurances, test_indices)\n",
    "cal_first_opensource_semantic_ids = utils.split(opensource_semantic_ids, cal_first_indices)\n",
    "cal_second_opensource_semantic_ids = utils.split(opensource_semantic_ids, cal_second_indices)\n",
    "test_opensource_semantic_ids = utils.split(opensource_semantic_ids, test_indices)\n",
    "cal_first_queries = utils.split(queries, cal_first_indices)\n",
    "cal_second_queries = utils.split(queries, cal_second_indices)\n",
    "test_queries = utils.split(queries, test_indices)\n",
    "cal_first_opensource_answers = utils.split(opensource_answers, cal_first_indices)\n",
    "cal_second_opensource_answers = utils.split(opensource_answers, cal_second_indices)\n",
    "test_opensource_answers = utils.split(opensource_answers, test_indices)\n",
    "cal_first_opensource_probs = utils.split(opensource_probs, cal_first_indices)\n",
    "cal_second_opensource_probs = utils.split(opensource_probs, cal_second_indices)\n",
    "test_opensource_probs = utils.split(opensource_probs, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "                                        use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute coverage rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(\n",
    "        retrieved_true_scores_list, opensource_true_scores_list,\n",
    "        retrieved_thr, qa_thr):\n",
    "\n",
    "    includes = []\n",
    "    for idx, (retrieved_true_score, opensource_true_score) in enumerate(zip(retrieved_true_scores_list, opensource_true_scores_list)):\n",
    "#         if idx > 20:\n",
    "        opensource_true_score = np.max(opensource_true_score)\n",
    "        include = True if (retrieved_true_score >= retrieved_thr and \n",
    "                           opensource_true_score >= qa_thr) \\\n",
    "                       else False\n",
    "        includes.append(include)\n",
    "    return includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA (I am not certain) threshold 0.7666666666666667\n",
      "validation qa coverage 0.44660194174757284\n",
      "test qa coverage 0.4109090909090909\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.6\n",
    "cal_first_scores = []\n",
    "for probs in cal_first_opensource_probs:\n",
    "    cal_first_scores.append(np.max(list(probs.values())))\n",
    "opensource_qa_thr = utils.compute_threshold(cal_first_scores, alpha=1-alpha)\n",
    "print('QA (I am not certain) threshold', opensource_qa_thr)\n",
    "\n",
    "cal_second_scores = []\n",
    "for probs in cal_second_opensource_probs:\n",
    "    cal_second_scores.append(np.max(list(probs.values())))\n",
    "qa_coverage = np.mean(np.array(cal_second_scores) <= opensource_qa_thr)\n",
    "print('validation qa coverage', qa_coverage)\n",
    "\n",
    "test_scores = []\n",
    "for probs in test_opensource_probs:\n",
    "    test_scores.append(np.max(list(probs.values())))\n",
    "qa_coverage = np.mean(np.array(test_scores) <= opensource_qa_thr)\n",
    "print('test qa coverage', qa_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
